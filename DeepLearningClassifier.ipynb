{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df['y']=le.fit_transform(df['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[day, every, happened, but]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[good, rat, water, better, fitted, would, cani...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[thing, lot, fix, trying, least, right]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[job, use, simply, could, wished, she]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[said, government, told, go, patrick, insisted...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12939</th>\n",
       "      <td>[stay, want, wife, baby, new, find, imagined, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12940</th>\n",
       "      <td>[father, say, back, come, reason, give, good, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12941</th>\n",
       "      <td>[i, sure, i, nowhere, out, consistently, writi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12942</th>\n",
       "      <td>[easy, live, place, new, find, decided, they]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12943</th>\n",
       "      <td>[mailbox, take, letter, open, needed, now]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12944 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       x  y\n",
       "0                            [day, every, happened, but]  1\n",
       "1      [good, rat, water, better, fitted, would, cani...  1\n",
       "2                [thing, lot, fix, trying, least, right]  1\n",
       "3                 [job, use, simply, could, wished, she]  1\n",
       "4      [said, government, told, go, patrick, insisted...  1\n",
       "...                                                  ... ..\n",
       "12939  [stay, want, wife, baby, new, find, imagined, ...  2\n",
       "12940  [father, say, back, come, reason, give, good, ...  2\n",
       "12941  [i, sure, i, nowhere, out, consistently, writi...  2\n",
       "12942      [easy, live, place, new, find, decided, they]  2\n",
       "12943         [mailbox, take, letter, open, needed, now]  2\n",
       "\n",
       "[12944 rows x 2 columns]"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    return tokens[::-1]\n",
    "\n",
    "df['x'] = df['x'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield text\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(df['x']), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                     [17, 182, 380, 169]\n",
      "1    [10, 3076, 51, 95, 2007, 8, 1948, 1]\n",
      "2            [14, 44, 3696, 181, 253, 74]\n",
      "3               [96, 89, 255, 11, 505, 5]\n",
      "4      [35, 239, 163, 30, 1434, 980, 169]\n",
      "Name: x, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def encode_text(text):\n",
    "    return [vocab[token] for token in text]\n",
    "df['x'] = df['x'].apply(encode_text)\n",
    "print(df['x'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_batch(batch):\n",
    "    text_list, label_list = [], []\n",
    "    for (_text, _label) in batch:\n",
    "        text_list.append(torch.tensor(_text, dtype=torch.int64))\n",
    "        label_list.append(torch.tensor(_label, dtype=torch.int64))\n",
    "    text_padded = pad_sequence(text_list, batch_first=True, padding_value=0)\n",
    "    label_tensor = torch.tensor(label_list, dtype=torch.int64)\n",
    "    return text_padded, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataframe.iloc[idx]['x']\n",
    "        label = self.dataframe.iloc[idx]['y']\n",
    "        return text, label\n",
    "\n",
    "train_dataset = TextDataset(train_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataset = TextDataset(test_data)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TextClassifierLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim,num_layers=2,def_dropout=0.5):\n",
    "        super(TextClassifierLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True,bidirectional=True,num_layers=num_layers, dropout=def_dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        out = self.fc(hidden[-1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x <class 'str'>\n",
      "y <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "for i in train_data:\n",
    "    print(i,type(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Training Accuracy: 24.84%\n",
      "Epoch [2/5], Training Accuracy: 26.85%\n",
      "Epoch [3/5], Training Accuracy: 28.45%\n",
      "Epoch [4/5], Training Accuracy: 30.12%\n",
      "Epoch [5/5], Training Accuracy: 31.52%\n",
      "Epoch [6/5], Training Accuracy: 33.10%\n",
      "Epoch [7/5], Training Accuracy: 34.69%\n",
      "Epoch [8/5], Training Accuracy: 36.25%\n",
      "Epoch [9/5], Training Accuracy: 37.86%\n",
      "Epoch [10/5], Training Accuracy: 39.41%\n",
      "Epoch [11/5], Training Accuracy: 40.98%\n",
      "Epoch [12/5], Training Accuracy: 42.48%\n",
      "Epoch [13/5], Training Accuracy: 43.95%\n",
      "Epoch [14/5], Training Accuracy: 45.39%\n",
      "Epoch [15/5], Training Accuracy: 46.76%\n",
      "Epoch [16/5], Training Accuracy: 48.06%\n",
      "Epoch [17/5], Training Accuracy: 49.34%\n",
      "Epoch [18/5], Training Accuracy: 50.60%\n",
      "Epoch [19/5], Training Accuracy: 51.73%\n",
      "Epoch [20/5], Training Accuracy: 52.86%\n",
      "Epoch [21/5], Training Accuracy: 53.93%\n",
      "Epoch [22/5], Training Accuracy: 54.97%\n",
      "Epoch [23/5], Training Accuracy: 55.98%\n",
      "Epoch [24/5], Training Accuracy: 56.96%\n",
      "Epoch [25/5], Training Accuracy: 57.87%\n",
      "Epoch [26/5], Training Accuracy: 58.76%\n",
      "Epoch [27/5], Training Accuracy: 59.62%\n",
      "Epoch [28/5], Training Accuracy: 60.43%\n",
      "Epoch [29/5], Training Accuracy: 61.24%\n",
      "Epoch [30/5], Training Accuracy: 61.98%\n",
      "Epoch [31/5], Training Accuracy: 62.72%\n",
      "Epoch [32/5], Training Accuracy: 63.41%\n",
      "Epoch [33/5], Training Accuracy: 64.09%\n",
      "Epoch [34/5], Training Accuracy: 64.74%\n",
      "Epoch [35/5], Training Accuracy: 65.37%\n",
      "Epoch [36/5], Training Accuracy: 65.97%\n",
      "Epoch [37/5], Training Accuracy: 66.54%\n",
      "Epoch [38/5], Training Accuracy: 67.10%\n",
      "Epoch [39/5], Training Accuracy: 67.63%\n",
      "Epoch [40/5], Training Accuracy: 68.16%\n",
      "Epoch [41/5], Training Accuracy: 68.66%\n",
      "Epoch [42/5], Training Accuracy: 69.14%\n",
      "Epoch [43/5], Training Accuracy: 69.61%\n",
      "Epoch [44/5], Training Accuracy: 70.06%\n",
      "Epoch [45/5], Training Accuracy: 70.49%\n",
      "Epoch [46/5], Training Accuracy: 70.93%\n",
      "Epoch [47/5], Training Accuracy: 71.33%\n",
      "Epoch [48/5], Training Accuracy: 71.72%\n",
      "Epoch [49/5], Training Accuracy: 72.10%\n",
      "Epoch [50/5], Training Accuracy: 72.48%\n",
      "Epoch [51/5], Training Accuracy: 72.84%\n",
      "Epoch [52/5], Training Accuracy: 73.19%\n",
      "Epoch [53/5], Training Accuracy: 73.53%\n",
      "Epoch [54/5], Training Accuracy: 73.86%\n",
      "Epoch [55/5], Training Accuracy: 74.18%\n",
      "Epoch [56/5], Training Accuracy: 74.49%\n",
      "Epoch [57/5], Training Accuracy: 74.78%\n",
      "Epoch [58/5], Training Accuracy: 75.08%\n",
      "Epoch [59/5], Training Accuracy: 75.36%\n",
      "Epoch [60/5], Training Accuracy: 75.62%\n",
      "Epoch [61/5], Training Accuracy: 75.89%\n",
      "Epoch [62/5], Training Accuracy: 76.15%\n",
      "Epoch [63/5], Training Accuracy: 76.39%\n",
      "Epoch [64/5], Training Accuracy: 76.65%\n",
      "Epoch [65/5], Training Accuracy: 76.88%\n",
      "Epoch [66/5], Training Accuracy: 77.11%\n",
      "Epoch [67/5], Training Accuracy: 77.34%\n",
      "Epoch [68/5], Training Accuracy: 77.56%\n",
      "Epoch [69/5], Training Accuracy: 77.78%\n",
      "Epoch [70/5], Training Accuracy: 78.00%\n",
      "Epoch [71/5], Training Accuracy: 78.20%\n",
      "Epoch [72/5], Training Accuracy: 78.40%\n",
      "Epoch [73/5], Training Accuracy: 78.60%\n",
      "Epoch [74/5], Training Accuracy: 78.79%\n",
      "Epoch [75/5], Training Accuracy: 78.98%\n",
      "Epoch [76/5], Training Accuracy: 79.16%\n",
      "Epoch [77/5], Training Accuracy: 79.33%\n",
      "Epoch [78/5], Training Accuracy: 79.51%\n",
      "Epoch [79/5], Training Accuracy: 79.67%\n",
      "Epoch [80/5], Training Accuracy: 79.84%\n",
      "Epoch [81/5], Training Accuracy: 80.00%\n",
      "Epoch [82/5], Training Accuracy: 80.15%\n",
      "Epoch [83/5], Training Accuracy: 80.31%\n",
      "Epoch [84/5], Training Accuracy: 80.46%\n",
      "Epoch [85/5], Training Accuracy: 80.61%\n",
      "Epoch [86/5], Training Accuracy: 80.76%\n",
      "Epoch [87/5], Training Accuracy: 80.90%\n",
      "Epoch [88/5], Training Accuracy: 81.04%\n",
      "Epoch [89/5], Training Accuracy: 81.18%\n",
      "Epoch [90/5], Training Accuracy: 81.31%\n",
      "Epoch [91/5], Training Accuracy: 81.44%\n",
      "Epoch [92/5], Training Accuracy: 81.57%\n",
      "Epoch [93/5], Training Accuracy: 81.70%\n",
      "Epoch [94/5], Training Accuracy: 81.82%\n",
      "Epoch [95/5], Training Accuracy: 81.94%\n",
      "Epoch [96/5], Training Accuracy: 82.06%\n",
      "Epoch [97/5], Training Accuracy: 82.18%\n",
      "Epoch [98/5], Training Accuracy: 82.29%\n",
      "Epoch [99/5], Training Accuracy: 82.40%\n",
      "Epoch [100/5], Training Accuracy: 82.52%\n"
     ]
    }
   ],
   "source": [
    "model = TextClassifierLSTM(vocab_size=len(vocab), embed_dim=32, hidden_dim=64, output_dim=5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "correct_train = 0\n",
    "total_train = 0\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    for texts, labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    print(f'Epoch [{epoch+1}/5], Training Accuracy: {train_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569 2589\n",
      "Validation Accuracy: 21.97759752800309%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for texts, labels in test_dataloader:\n",
    "        outputs = model(texts)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(correct,total)\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Validation Accuracy: {accuracy}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
