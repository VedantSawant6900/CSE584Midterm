{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9V6hHASDUks",
        "outputId": "d931d3e6-481f-4cfa-9693-75d53834908d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /Users/vishakhasawant/opt/anaconda3/envs/LLM/lib/python3.9/site-packages (4.45.1)\n",
            "Requirement already satisfied: numpy in /Users/vishakhasawant/opt/anaconda3/envs/LLM/lib/python3.9/site-packages (1.26.0)\n",
            "Requirement already satisfied: pandas in /Users/vishakhasawant/opt/anaconda3/envs/LLM/lib/python3.9/site-packages (2.2.3)\n",
            "Requirement already satisfied: scikit-learn in /Users/vishakhasawant/opt/anaconda3/envs/LLM/lib/python3.9/site-packages (1.5.2)\n",
            "Requirement already satisfied: tqdm in /Users/vishakhasawant/opt/anaconda3/envs/LLM/lib/python3.9/site-packages (4.65.0)\n",
            "Requirement already satisfied: chardet in /Users/vishakhasawant/opt/anaconda3/envs/LLM/lib/python3.9/site-packages (5.2.0)\n",
            "Requirement already satisfied: filelock in /Users/vishakhasawant/opt/anaconda3/envs/LLM/lib/python3.9/site-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/vishakhasawant/opt/anaconda3/envs/LLM/lib/python3.9/site-packages (from transformers) (0.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/vishakhasawant/opt/anaconda3/envs/LLM/lib/python3.9/site-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/vishakhasawant/opt/anaconda3/envs/LLM/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/vishakhasawant/opt/anaconda3/envs/LLM/lib/python3.9/site-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /Users/vishakhasawant/opt/anaconda3/envs/LLM/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /Users/vishakhasawant/opt/anaconda3/envs/LLM/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/vishakhasawant/opt/anaconda3/envs/LLM/lib/python3.9/site-packages (from transformers) (0.20.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/vishakhasawant/opt/anaconda3/envs/LLM/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/vishakhasawant/opt/anaconda3/envs/LLM/lib/python3.9/site-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/vishakhasawant/opt/anaconda3/envs/LLM/lib/python3.9/site-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /Users/vishakhasawant/opt/anaconda3/envs/LLM/lib/python3.9/site-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /Users/vishakhasawant/opt/anaconda3/envs/LLM/lib/python3.9/site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/vishakhasawant/opt/anaconda3/envs/LLM/lib/python3.9/site-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/vishakhasawant/opt/anaconda3/envs/LLM/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/vishakhasawant/opt/anaconda3/envs/LLM/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /Users/vishakhasawant/opt/anaconda3/envs/LLM/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/vishakhasawant/opt/anaconda3/envs/LLM/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/vishakhasawant/opt/anaconda3/envs/LLM/lib/python3.9/site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/vishakhasawant/opt/anaconda3/envs/LLM/lib/python3.9/site-packages (from requests->transformers) (1.26.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/vishakhasawant/opt/anaconda3/envs/LLM/lib/python3.9/site-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers numpy pandas scikit-learn tqdm chardet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "JUDHLgTpOpj7"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# os.chdir('/content/drive/MyDrive/ML_Midterm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FvCXuRZcvsQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HJgLAaJ6OqG_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('dataset.csv')\n",
        "texts = data['x']\n",
        "labels = data['y']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dbtrd3WOO3xu"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "labels = encoder.fit_transform(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Ud08URCPO87U"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
        "    texts, labels, stratify=labels, test_size=0.3\n",
        ")\n",
        "\n",
        "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
        "    temp_texts, temp_labels, stratify=temp_labels, test_size=0.5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Erhv9d1xPD9Z",
        "outputId": "4e27405e-74b9-4dde-a104-36288a78ab59"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def encode_texts(texts):\n",
        "    return tokenizer(\n",
        "        texts.tolist(),\n",
        "        add_special_tokens=True,\n",
        "        max_length=32,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "train_encodings = encode_texts(train_texts)\n",
        "val_encodings = encode_texts(val_texts)\n",
        "test_encodings = encode_texts(test_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "kuFYgqfGPHPl"
      },
      "outputs": [],
      "source": [
        "def create_attention_masks(encodings):\n",
        "    return [\n",
        "        [float(i > 0) for i in seq] for seq in encodings['input_ids']\n",
        "    ]\n",
        "\n",
        "train_masks = create_attention_masks(train_encodings)\n",
        "val_masks = create_attention_masks(val_encodings)\n",
        "test_masks = create_attention_masks(test_encodings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "vVbzkCEvPJdf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "train_dataset = TensorDataset(\n",
        "    train_encodings['input_ids'],\n",
        "    torch.tensor(train_masks),\n",
        "    torch.tensor(train_labels)\n",
        ")\n",
        "\n",
        "val_dataset = TensorDataset(\n",
        "    val_encodings['input_ids'],\n",
        "    torch.tensor(val_masks),\n",
        "    torch.tensor(val_labels)\n",
        ")\n",
        "\n",
        "test_dataset = TensorDataset(\n",
        "    test_encodings['input_ids'],\n",
        "    torch.tensor(test_masks),\n",
        "    torch.tensor(test_labels)\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86ro0jfcPSPP",
        "outputId": "0a62a9cf-3716-4bf2-9093-d151e78776b2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=5,  # Adjust this based on your classification task\n",
        "    output_attentions=False,\n",
        "    output_hidden_states=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "r8Ptijz8P3AE"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "batch_size = 1000\n",
        "num_epochs = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLiVkN3FP8Os",
        "outputId": "802aea25-9e31-46d8-ecd6-5b29fde11984"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4X5aspGQBs9",
        "outputId": "e5394983-5947-415b-ccfe-699871f36d7f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/vishakhasawant/opt/anaconda3/envs/LLM/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "total_steps = len(train_loader) * num_epochs\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "sqzoQch4QFMZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting epoch  0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(\"Starting epoch \",epoch)\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        b_input_ids, b_input_mask, b_labels = tuple(t for t in batch)\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    print(f'Epoch {epoch+1}, Loss: {avg_train_loss}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaxLQeXeQV8A"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "import numpy as np\n",
        "\n",
        "predictions, true_labels = [], []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1-gPHdyQZHH"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        b_input_ids, b_input_mask, b_labels = tuple(t.to('cuda') for t in batch)\n",
        "\n",
        "        outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Store predictions and true labels\n",
        "        predictions.append(logits)\n",
        "        true_labels.append(label_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeS_FxkoQa6v"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Concatenate all predictions and true labels\n",
        "flat_predictions = np.concatenate(predictions, axis=0)\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(flat_true_labels, flat_predictions)\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(flat_true_labels, flat_predictions, average='weighted')\n",
        "\n",
        "print(f'Validation Accuracy: {accuracy}')\n",
        "print(f'Validation Precision: {precision}')\n",
        "print(f'Validation Recall: {recall}')\n",
        "print(f'Validation F1-Score: {f1}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
